{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f23e41",
   "metadata": {},
   "source": [
    "# RQ2 (N=160, All 19 Properties)\n",
    "This notebook rebuilds RQ2 over all 160 reports using **all 19 canonical properties**. It augments the base CSV with text-derived signals parsed from the enrichment spreadsheets (Report Structure, Methodology, Severity definitions, Knowledge) to recover additional sections, runs FCA over the full set, mines implication rules, defines style families, and exports CSVs and a providersÃ—families heatmap.\n",
    "\n",
    "> Note: `Detailed Findings` is universal in this run and is **not** used to define families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e956ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "base = Path(\"/mnt/data\")\n",
    "out = base / \"rq2_160_all19_outputs\"\n",
    "out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Load base 160 CSV\n",
    "base_df = pd.read_csv(base / \"all_reports_coded_with_links.csv\")\n",
    "base_df.columns = [c.strip() for c in base_df.columns]\n",
    "for col in base_df.columns:\n",
    "    if col in [\"Provider\",\"Project\",\"Link\",\"Coder\"]:\n",
    "        continue\n",
    "    base_df[col] = pd.to_numeric(base_df[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# === Load enrichment sheets\n",
    "def load_sheet(path):\n",
    "    xls = pd.ExcelFile(path)\n",
    "    df = pd.read_excel(xls, sheet_name=xls.sheet_names[0])\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    return df\n",
    "\n",
    "xlsx_paths = [\n",
    "    base / \"audit_reports_extraction.xlsx\",\n",
    "    base / \"audit_reports_extracted.xlsx\",\n",
    "    base / \"remaining_audit_reports_extraction.xlsx\",\n",
    "]\n",
    "extra_frames = []\n",
    "for p in xlsx_paths:\n",
    "    if p.exists():\n",
    "        extra_frames.append(load_sheet(p))\n",
    "\n",
    "def unify_cols(df):\n",
    "    df = df.copy()\n",
    "    if \"Project Name\" in df.columns and \"Project_Name\" not in df.columns:\n",
    "        df[\"Project_Name\"] = df[\"Project Name\"]\n",
    "    if \"Link\" not in df.columns:\n",
    "        for c in df.columns:\n",
    "            if c.lower() in [\"url\",\"report link\",\"report_url\"]:\n",
    "                df[\"Link\"] = df[c]; break\n",
    "    for col in [\"Provider\",\"Project_Name\",\"Link\",\"Report Structure\",\"Report_Structure\",\"Knowledge\",\"Methodology\",\"Severity Level Definitions\",\"Severity_Level_Definitions\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    return df\n",
    "\n",
    "def row_text_blob(row):\n",
    "    parts = []\n",
    "    for c in [\"Report Structure\",\"Report_Structure\",\"Knowledge\",\"Methodology\",\"Severity Level Definitions\",\"Severity_Level_Definitions\"]:\n",
    "        v = str(row.get(c,\"\"))\n",
    "        if pd.notna(v) and v.strip():\n",
    "            parts.append(v)\n",
    "    return \" | \".join(parts).lower()\n",
    "\n",
    "extras = []\n",
    "for df in extra_frames:\n",
    "    dfu = unify_cols(df)\n",
    "    dfu[\"__text__\"] = dfu.apply(row_text_blob, axis=1)\n",
    "    dfu[\"Link_key\"] = dfu[\"Link\"].astype(str).str.strip().str.lower()\n",
    "    dfu[\"Provider_key\"] = dfu[\"Provider\"].astype(str).str.strip().str.lower()\n",
    "    dfu[\"Project_key\"]  = dfu[\"Project_Name\"].astype(str).str.strip().str.lower()\n",
    "    extras.append(dfu)\n",
    "\n",
    "base_df[\"Link_key\"]    = base_df[\"Link\"].astype(str).str.strip().str.lower()\n",
    "base_df[\"Provider_key\"] = base_df[\"Provider\"].astype(str).str.strip().str.lower()\n",
    "base_df[\"Project_key\"]  = base_df[\"Project\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# === Patterns for all 19 properties (Detailed Findings set to 1 later)\n",
    "pat = {\n",
    "    \"Executive Summary\": r\"\\bexecutive (summary|overview)\\b|technical summary\",\n",
    "    \"Scope\": r\"\\bscope\\b|out of scope\",\n",
    "    \"System/Protocol Overview\": r\"\\b(system|protocol|project)\\s+(overview|architecture|design)|\\boverview\\b\",\n",
    "    \"Versioning & References\": r\"\\bcommit\\b|\\brevision\\b|\\bhash\\b|\\brepository\\b|\\bversion\\b|\\breferences\\b|\\bref\\.\\b|appendix\",\n",
    "    \"Methodology / Procedure\": r\"\\bmethodology\\b|\\bmethods?\\b|\\bprocedure\\b|\\bapproach\\b\",\n",
    "    \"Tools & Automation\": r\"\\btool(s|ing)?\\b|slither|mythril|oyente|manticore|echidna|foundry|halmos|static analysis|automation\",\n",
    "    \"Severity Model / Risk Classification\": r\"\\bseverity (definitions?|matrix|categories)\\b|risk classification|cvss|bvss\",\n",
    "    \"Findings Summary (Table)\": r\"\\bfindings (summary|table)\\b|summary table|vulnerability summary|severity table\",\n",
    "    \"Recommendations / Remediation Guidance\": r\"\\brecommend(ation|ations|ed|s)\\b|\\bmitigat(e|ion|ions)\\b|\\bremediation\\b|\\bsuggest(s|ed|ions?)\\b\",\n",
    "    \"Status of Findings / Verification\": r\"\\bstatus\\b|fixed|partially fixed|acknowledged|verification|re-?check|final review|post-?audit|fix review\",\n",
    "    \"Threat / Trust Model\": r\"\\bthreat model\\b|\\btrust model\\b|\\bassumption(s)?\\b|\\bsecurity specification\\b\",\n",
    "    \"Privileged Roles & Centralisation Risks\": r\"\\bprivileg(e|ed)\\b|admin key|owner privileg|centralis(z|s)ation|centraliz(s|z)ation|centralization risk\",\n",
    "    \"Risk Taxonomy / Terminology & References\": r\"\\bterminology\\b|\\btaxonomy\\b|\\bglossary\\b|\\bdefinitions?\\b|\\bsecurity specification\\b\",\n",
    "    \"Timeline / Engagement Details\": r\"\\btimeline\\b|engagement (timeline|period)|review period|start date|end date\",\n",
    "    \"Testing / Coverage Quality\": r\"\\bcoverage\\b|unit testing|tests? coverage|test quality|project coverage|fuzz(ing)? coverage\",\n",
    "    \"Documentation Quality\": r\"\\bdocumentation\\b.*\\bquality\\b|documentation quality|docs quality|documentation assessment\",\n",
    "    \"Upgradeability / Governance Controls\": r\"\\bgovernance\\b|upgradeab(le|ility)|upgradeable|proxy|time[- ]?lock|governance controls\",\n",
    "    \"Limitations / Disclaimers / Non-Goals\": r\"\\bdisclaimer(s)?\\b|limitation(s)?|non[- ]?goals|not a guarantee|no warranty\",\n",
    "}\n",
    "\n",
    "props19 = [\n",
    "    'Executive Summary','Scope','System/Protocol Overview','Versioning & References',\n",
    "    'Methodology / Procedure','Tools & Automation','Severity Model / Risk Classification',\n",
    "    'Findings Summary (Table)','Detailed Findings','Recommendations / Remediation Guidance',\n",
    "    'Status of Findings / Verification','Threat / Trust Model',\n",
    "    'Privileged Roles & Centralisation Risks','Risk Taxonomy / Terminology & References',\n",
    "    'Timeline / Engagement Details','Testing / Coverage Quality','Documentation Quality',\n",
    "    'Upgradeability / Governance Controls','Limitations / Disclaimers / Non-Goals'\n",
    "]\n",
    "\n",
    "def parse_flags(text):\n",
    "    flags = {}\n",
    "    for p, rgx in pat.items():\n",
    "        flags[p] = 1 if re.search(rgx, text, flags=re.I) else 0\n",
    "    return flags\n",
    "\n",
    "parsed_records = []\n",
    "for dfu in extras:\n",
    "    for _, row in dfu.iterrows():\n",
    "        text = row[\"__text__\"]\n",
    "        flags = parse_flags(text)\n",
    "        rec = {\"Link_key\": row[\"Link_key\"], \"Provider_key\": row[\"Provider_key\"], \"Project_key\": row[\"Project_key\"]}\n",
    "        rec.update(flags)\n",
    "        parsed_records.append(rec)\n",
    "\n",
    "parsed_df = pd.DataFrame(parsed_records) if parsed_records else pd.DataFrame(columns=[\"Link_key\",\"Provider_key\",\"Project_key\"]+list(pat.keys()))\n",
    "agg_by_link = parsed_df.groupby(\"Link_key\").max().reset_index()\n",
    "agg_by_pp   = parsed_df.groupby([\"Provider_key\",\"Project_key\"]).max().reset_index()\n",
    "\n",
    "merged = base_df.merge(agg_by_link, on=\"Link_key\", how=\"left\", suffixes=(\"\",\"_parsed\"))\n",
    "merged = merged.merge(agg_by_pp, on=[\"Provider_key\",\"Project_key\"], how=\"left\", suffixes=(\"\",\"_pp\"))\n",
    "\n",
    "final = pd.DataFrame(index=merged.index)\n",
    "final[\"Provider\"] = merged[\"Provider\"]\n",
    "final[\"Project\"]  = merged[\"Project\"]\n",
    "final[\"Link\"]     = merged[\"Link\"]\n",
    "\n",
    "base_map = {\n",
    "    \"Executive Summary\": [\"Summary\"],\n",
    "    \"Scope\": [\"Scope\"],\n",
    "    \"Methodology / Procedure\": [\"Methodology\"],\n",
    "    \"Tools & Automation\": [\"Automatic Detection\",\"Detection Tools\",\"Techniques Used Mentions\"],\n",
    "    \"Recommendations / Remediation Guidance\": [\"Recommendations\"],\n",
    "    \"Status of Findings / Verification\": [\"Fix Review Included\",\"Status of Findings\"],\n",
    "    \"Testing / Coverage Quality\": [\"Test Quality\"],\n",
    "    \"Documentation Quality\": [\"Documentation Quality\"],\n",
    "    \"Limitations / Disclaimers / Non-Goals\": [\"Limitations Mentioned\"],\n",
    "}\n",
    "\n",
    "for p in props19:\n",
    "    if p == \"Detailed Findings\":\n",
    "        final[p] = 1\n",
    "        continue\n",
    "    base_val = 0\n",
    "    for src in base_map.get(p, []):\n",
    "        if src in merged.columns:\n",
    "            base_val = base_val | merged[src].fillna(0).astype(int)\n",
    "    parsed_link_col = p\n",
    "    parsed_pp_col   = p + \"_pp\"\n",
    "    if parsed_link_col not in merged.columns: merged[parsed_link_col] = 0\n",
    "    if parsed_pp_col not in merged.columns:   merged[parsed_pp_col] = 0\n",
    "    final[p] = (base_val | merged[parsed_link_col].fillna(0).astype(int) | merged[parsed_pp_col].fillna(0).astype(int)).astype(int)\n",
    "\n",
    "context19 = final.copy()\n",
    "context19.to_csv(out/\"RQ2_FCA_context_160_all19.csv\", index=False)\n",
    "\n",
    "# --- Prevalence\n",
    "attr19 = pd.DataFrame({\"Property\": props19, \"Support\": [int(context19[p].sum()) for p in props19]})\n",
    "attr19[\"Coverage(%)\"] = (attr19[\"Support\"]/len(context19)*100).round(1)\n",
    "attr19 = attr19.sort_values(\"Support\", ascending=False)\n",
    "attr19.to_csv(out/\"RQ2_attribute_support_coverage_160_all19.csv\", index=False)\n",
    "\n",
    "# --- FCA (Next-Closure)\n",
    "A = props19\n",
    "obj_attrs_all = [{a for a in A if context19.loc[i, a]==1} for i in context19.index]\n",
    "\n",
    "def extent_of_intent(intent):\n",
    "    return set(i for i,attrs in enumerate(obj_attrs_all) if intent.issubset(attrs))\n",
    "def intent_of_extent(ext):\n",
    "    if not ext: return set(A)\n",
    "    inter = set(A)\n",
    "    for i in ext: inter &= obj_attrs_all[i]\n",
    "    return inter\n",
    "def closure_intent(X): return intent_of_extent(extent_of_intent(X))\n",
    "\n",
    "def next_closure(current):\n",
    "    curr_bits = [1 if a in current else 0 for a in A]\n",
    "    for i in range(len(A)-1, -1, -1):\n",
    "        if curr_bits[i]==0:\n",
    "            y = set(current); y.add(A[i]); y = closure_intent(y)\n",
    "            ok = True\n",
    "            for j in range(i):\n",
    "                if (A[j] in y) != (curr_bits[j]==1): ok = False; break\n",
    "            if ok: return y\n",
    "    return None\n",
    "\n",
    "concepts = []; seen = set()\n",
    "X = closure_intent(set())\n",
    "while True:\n",
    "    key = tuple(sorted(X))\n",
    "    if key not in seen:\n",
    "        seen.add(key)\n",
    "        ext = extent_of_intent(X)\n",
    "        concepts.append({\"IntentSize\": len(X), \"Support\": len(ext), \"Intent\": \"; \".join(sorted(X))})\n",
    "    nxt = next_closure(X)\n",
    "    if nxt is None or nxt == X: break\n",
    "    X = nxt\n",
    "\n",
    "concepts_all19_df = pd.DataFrame(concepts).sort_values([\"Support\",\"IntentSize\"], ascending=[False,True]).reset_index(drop=True)\n",
    "concepts_all19_df.to_csv(out/\"RQ2_FCA_concepts_160_all19.csv\", index=False)\n",
    "\n",
    "# --- Implications and DG-like pruned set\n",
    "impl = []\n",
    "def sup(S): return len(extent_of_intent(set(S)))\n",
    "for r in range(1,4):\n",
    "    for comb in combinations(A, r):\n",
    "        c = closure_intent(set(comb))\n",
    "        if set(c) > set(comb):\n",
    "            impl.append({\"Premise\": \"; \".join(comb), \"Implied\": \"; \".join(sorted(set(c)-set(comb))), \"Support\": sup(comb)})\n",
    "imp_all19_df = pd.DataFrame(impl).sort_values(\"Support\", ascending=False).reset_index(drop=True)\n",
    "imp_all19_df.to_csv(out/\"RQ2_FCA_implications_160_all19.csv\", index=False)\n",
    "\n",
    "imp_all19_df[\"Pset\"] = imp_all19_df[\"Premise\"].apply(lambda s: frozenset(x.strip() for x in s.split(\";\")))\n",
    "imp_all19_df[\"Cset\"] = imp_all19_df[\"Implied\"].apply(lambda s: frozenset(x.strip() for x in s.split(\";\") if x.strip()))\n",
    "keep = [True]*len(imp_all19_df)\n",
    "for i in range(len(imp_all19_df)):\n",
    "    if not keep[i]: continue\n",
    "    Pi, Ci = imp_all19_df.loc[i,\"Pset\"], imp_all19_df.loc[i,\"Cset\"]\n",
    "    for j in range(i+1, len(imp_all19_df)):\n",
    "        if not keep[j]: continue\n",
    "        Pj, Cj = imp_all19_df.loc[j,\"Pset\"], imp_all19_df.loc[j,\"Cset\"]\n",
    "        if Pi.issubset(Pj) and Cj.issubset(Ci): keep[j] = False\n",
    "dg_like_all19 = imp_all19_df.loc[keep, [\"Premise\",\"Implied\",\"Support\"]].reset_index(drop=True)\n",
    "dg_like_all19.to_csv(out/\"RQ2_DG_canonical_basis_like_160_all19.csv\", index=False)\n",
    "\n",
    "# --- Families\n",
    "families_all19 = {\n",
    "    \"Executive-Packaged\": {\"all_of\": [\"Executive Summary\"], \"any_of\": [\"Severity Model / Risk Classification\",\"Findings Summary (Table)\"]},\n",
    "    \"Core-Engineering\": {\"all_of\": [\"System/Protocol Overview\"], \"any_of\": [\"Methodology / Procedure\",\"Tools & Automation\",\"Testing / Coverage Quality\"]},\n",
    "    \"Remediation-First\": {\"all_of\": [], \"any_of\": [\"Recommendations / Remediation Guidance\",\"Status of Findings / Verification\"]},\n",
    "    \"Governance-Focused\": {\"all_of\": [], \"any_of\": [\"Threat / Trust Model\",\"Privileged Roles & Centralisation Risks\",\"Upgradeability / Governance Controls\"]},\n",
    "    \"Legal/Taxonomy-Heavy\": {\"all_of\": [\"Limitations / Disclaimers / Non-Goals\"], \"any_of\": [\"Risk Taxonomy / Terminology & References\",\"Severity Model / Risk Classification\"]}\n",
    "}\n",
    "def fam_memberships_all19(row):\n",
    "    outm = {}\n",
    "    for fam, rule in families_all19.items():\n",
    "        all_ok = all(row.get(a,0)==1 for a in rule[\"all_of\"])\n",
    "        any_ok = (len(rule[\"any_of\"])==0) or any(row.get(a,0)==1 for a in rule[\"any_of\"])\n",
    "        outm[fam] = int(all_ok and any_ok)\n",
    "    return pd.Series(outm)\n",
    "\n",
    "fam_mat_19 = context19.apply(fam_memberships_all19, axis=1)\n",
    "fam_cov_19 = fam_mat_19.sum().to_frame(\"ReportsWithFamily\")\n",
    "fam_cov_19[\"Coverage(%)\"] = (fam_cov_19[\"ReportsWithFamily\"]/len(context19)*100).round(1)\n",
    "fam_cov_19 = fam_cov_19.sort_values(\"ReportsWithFamily\", ascending=False)\n",
    "fam_cov_19.reset_index().rename(columns={\"index\":\"Family\"}).to_csv(out/\"RQ2_family_coverage_160_all19.csv\", index=False)\n",
    "\n",
    "prov_soft_19 = pd.concat([context19[[\"Provider\",\"Project\"]], fam_mat_19], axis=1)\n",
    "prov_soft_ratios_19 = prov_soft_19.groupby(\"Provider\")[list(families_all19.keys())].mean().round(2)\n",
    "prov_soft_counts_19 = prov_soft_19.groupby(\"Provider\").size().to_frame(\"N_reports\")\n",
    "prov_soft_table_19 = prov_soft_counts_19.join(prov_soft_ratios_19)\n",
    "prov_soft_table_19.to_csv(out/\"RQ2_provider_family_soft_160_all19.csv\")\n",
    "\n",
    "plt.figure(figsize=(10, max(4, 0.35*len(prov_soft_ratios_19))))\n",
    "plt.imshow(prov_soft_ratios_19.values, aspect='auto')\n",
    "plt.xticks(range(len(prov_soft_ratios_19.columns)), prov_soft_ratios_19.columns, rotation=45, ha='right')\n",
    "plt.yticks(range(len(prov_soft_ratios_19.index)), prov_soft_ratios_19.index)\n",
    "plt.colorbar()\n",
    "plt.title(\"Soft adoption of style families by provider (N=160, all 19 properties)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(out/\"figure_soft_adoption_heatmap_160_all19.png\", dpi=200)\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
